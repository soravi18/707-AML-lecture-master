{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding data\n",
    "\n",
    "In general, SciKit learn only processes numerical data (with some rare exceptions).  Therefore it's necessary to encode categorical data prior to the application of any of the ML algorithms from sklearn.  However, there are several considerations that will impact your data processing.\n",
    "\n",
    "#### Label Encoding: When Not to Use It\n",
    "\n",
    "Label encoding transforms categorical data into integer values (e.g., 'red' -> 0, 'blue' -> 1, 'green' -> 2). While this is convenient, it's not suitable for non-ordinal categorical variables (variables that don't have a natural order) when using certain algorithms like Logistic Regression.  Note that label encoding is generally ok for Decision Trees (we'll talk about these later).\n",
    "\n",
    "##### Example: Why LabelEncoding Messes Up ML\n",
    "\n",
    "Consider a synthetic dataset with two clear clusters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 100\n",
    "sweet_juicy = np.random.uniform(2, 3, (n_samples, 2))\n",
    "not_sweet_not_juicy = np.random.uniform(1, 2, (n_samples, 2))\n",
    "\n",
    "# Assign colors (ordinal labels should coincide with the natural clusters)\n",
    "colors_sweet_juicy = np.random.choice(['Blue', 'Orange', 'Purple'], n_samples)\n",
    "colors_not_sweet_not_juicy = np.random.choice(['Green', 'Pink', 'Red'], n_samples)\n",
    "\n",
    "# Combine data\n",
    "X = np.vstack([sweet_juicy, not_sweet_not_juicy])\n",
    "colors = np.concatenate([colors_sweet_juicy, colors_not_sweet_not_juicy])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=['Sweetness', 'Juiciness'])\n",
    "df['Color'] = colors\n",
    "\n",
    "# Label encode (alphabetic)\n",
    "le1 = LabelEncoder()\n",
    "df['Color_auto_encoded'] = le1.fit_transform(df['Color'])\n",
    "# Label encode (manual)\n",
    "color_mapping = {'Blue': 0, 'Orange': 1, 'Purple': 2, 'Green': 3, 'Pink': 4, 'Red': 5}\n",
    "df['Color_manual_encoded'] = df['Color'].map(color_mapping)\n",
    "\n",
    "# Perform k-means clustering with auto-encoded labels\n",
    "kmeans_auto = KMeans(n_clusters=2, random_state=0)\n",
    "df['Cluster_auto_encoded'] = kmeans_auto.fit_predict(df[['Sweetness', 'Juiciness','Color_auto_encoded']])\n",
    "\n",
    "# Perform k-means clustering with manually assigned labels\n",
    "kmeans_manual = KMeans(n_clusters=2, random_state=0)\n",
    "df['Cluster_manual_encoded'] = kmeans_manual.fit_predict(df[['Sweetness', 'Juiciness','Color_manual_encoded']])\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].scatter(df['Sweetness'], df['Juiciness'], c=df['Cluster_auto_encoded'], cmap='viridis')\n",
    "axes[0].set_title('Cluster Label Encoded')\n",
    "axes[0].set_xlabel('Sweetness')\n",
    "axes[0].set_ylabel('Juiciness')\n",
    "\n",
    "axes[1].scatter(df['Sweetness'], df['Juiciness'], c=df['Cluster_manual_encoded'], cmap='viridis')\n",
    "axes[1].set_title('Cluster Manually Encoded')\n",
    "axes[1].set_xlabel('Sweetness')\n",
    "axes[1].set_ylabel('Juiciness')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, the auto-encoding assigns values in alphabetic order, which conflicts with the natural clustering in the other dimensions.  Reassigning the encoding differently leads to a different result, even though the data didn't change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### One-Hot Encoding: A Better Alternative\n",
    "\n",
    "For non-ordinal categorical variables, it's usually better to use one-hot encoding. This creates a binary column for each category and indicates the presence of the categories with a 1 or 0.  For example:\n",
    "\n",
    "##### Table with Categorical Column\n",
    "\n",
    "| Index | Color  |\n",
    "|-------|--------|\n",
    "| 0     | Red    |\n",
    "| 1     | Blue   |\n",
    "| 2     | Green  |\n",
    "| 3     | Red    |\n",
    "| 4     | Green  |\n",
    "\n",
    "##### Table after One-Hot Encoding\n",
    "\n",
    "| Index | Color_Red | Color_Blue | Color_Green |\n",
    "|-------|-----------|------------|-------------|\n",
    "| 0     | 1         | 0          | 0           |\n",
    "| 1     | 0         | 1          | 0           |\n",
    "| 2     | 0         | 0          | 1           |\n",
    "| 3     | 1         | 0          | 0           |\n",
    "| 4     | 0         | 0          | 1           |\n",
    "\n",
    "However, when one-hot encoding, you'll note that if you know two of the columns, you automatically know the value of the third.  This is sometimes referred to as the \"dummy variable trap,\" and creates something called \"multicolinearity,\" which cause problems for learners (like linear regression).  Therefore, it is usually the case that one of the columns is dropped, like:\n",
    "\n",
    "##### Table after One-Hot Encoding with `Color_Red` dropped.\n",
    "\n",
    "| Index | Color_Blue | Color_Green |\n",
    "|-------|------------|-------------|\n",
    "| 0     | 0          | 0           |\n",
    "| 1     | 1          | 0           |\n",
    "| 2     | 0          | 1           |\n",
    "| 3     | 0          | 0           |\n",
    "| 4     | 0          | 1           |\n",
    "\n",
    "Both `pd.get_dummies()` and Scikit-Learn's `OneHotEncoder` serve the same fundamental purpose: they convert categorical data into a format that can be provided to machine learning algorithms, typically in the form of one-hot encoding. However, they do have differences, and each comes with its own set of advantages and disadvantages.\n",
    "\n",
    "#### Flexibility with Data Types:\n",
    "\n",
    "- `pd.get_dummies()`: Handles string labels directly.\n",
    "- `OneHotEncoder`: Only handles string labels if you set `handle_unknown='ignore'`.\n",
    "\n",
    "#### Handling Unknown Categories:\n",
    "\n",
    "- `pd.get_dummies()`: Doesn't handle unknown categories in the test data by default. You have to manage this manually.\n",
    "- `OneHotEncoder`: Can handle unknown categories in test data by setting `handle_unknown='ignore'`.\n",
    "\n",
    "#### Output Type:\n",
    "\n",
    "- `pd.get_dummies()`: Returns a DataFrame.\n",
    "- `OneHotEncoder`: Returns a NumPy array or sparse matrix.\n",
    "\n",
    "#### Feature Names:\n",
    "\n",
    "- `pd.get_dummies()`: Directly gives a DataFrame with meaningful column names.\n",
    "- `OneHotEncoder`: Feature names can be retrieved, but it's not as straightforward as with `get_dummies()`.\n",
    "\n",
    "#### Imputation:\n",
    "\n",
    "- `pd.get_dummies()`: Doesn't support missing values.\n",
    "- `OneHotEncoder`: Doesn't support missing values either, but can work alongside Scikit-Learn's imputation tools, as it is part of the same ecosystem.\n",
    "\n",
    "#### Pipeline:\n",
    "\n",
    "- `pd.get_dummies()`: Can't be included in a Scikit-Learn `Pipeline`.\n",
    "- `OneHotEncoder`: Can be part of a Scikit-Learn `Pipeline`, making it more convenient for model selection and tuning.\n",
    "\n",
    "### Which is Preferred and Why?\n",
    "\n",
    "- **For Quick Exploration or Prototyping**: `pd.get_dummies()` is often quicker and easier to use for one-off tasks. \n",
    "- **For Production or Pipelined Tasks**: `OneHotEncoder` is usually the choice when building end-to-end machine learning pipelines or when you need better handling of edge cases, like unknown categories in test data.\n",
    "\n",
    "So, the \"best\" method depends on the specifics of what you're trying to accomplish. If you're working solely within the Pandas ecosystem for a quick data analysis, `pd.get_dummies()` might be all you need. But if you're developing a machine learning pipeline that will go into production, or if you want more control over the behavior of the encoder, then Scikit-Learn's `OneHotEncoder` is generally more suitable.  In this class, we'll focus on using sci-kit learn's OneHotEncoder.\n",
    "\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 100\n",
    "sweet_juicy = np.random.uniform(2, 3, (n_samples, 2))\n",
    "not_sweet_not_juicy = np.random.uniform(1, 2, (n_samples, 2))\n",
    "\n",
    "# Assign colors (ordinal labels should coincide with the natural clusters)\n",
    "colors_sweet_juicy = np.random.choice(['Blue', 'Orange', 'Purple'], n_samples)\n",
    "colors_not_sweet_not_juicy = np.random.choice(['Green', 'Pink', 'Red'], n_samples)\n",
    "\n",
    "# Combine data\n",
    "X = np.vstack([sweet_juicy, not_sweet_not_juicy])\n",
    "colors = np.concatenate([colors_sweet_juicy, colors_not_sweet_not_juicy])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=['Sweetness', 'Juiciness'])\n",
    "df['Color'] = colors\n",
    "\n",
    "# OneHotEncoding\n",
    "# Note the \"spare=False\" - this is necessary because we are converting the encoded matrix\n",
    "# back into a DataFrame\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "colors_one_hot = encoder.fit_transform(df[['Color']])\n",
    "df_one_hot = pd.DataFrame(colors_one_hot, columns=encoder.get_feature_names_out(['Color']))\n",
    "\n",
    "# Combine the original dataframe with one-hot-encoded dataframe\n",
    "df_combined = pd.concat([df, df_one_hot], axis=1)\n",
    "\n",
    "# Label encode (alphabetic)\n",
    "le1 = LabelEncoder()\n",
    "df['Color_auto_encoded'] = le1.fit_transform(df['Color'])\n",
    "\n",
    "# Perform k-means clustering with auto-encoded labels\n",
    "kmeans_auto = KMeans(n_clusters=2, random_state=0)\n",
    "df['Cluster_auto_encoded'] = kmeans_auto.fit_predict(df[['Sweetness', 'Juiciness','Color_auto_encoded']])\n",
    "\n",
    "\n",
    "# Performing k-means clustering with one-hot-encoded labels\n",
    "kmeans_one_hot = KMeans(n_clusters=2, random_state=0)\n",
    "df_combined['Cluster_one_hot_encoded'] = kmeans_one_hot.fit_predict(df_combined.drop(['Color'], axis=1))\n",
    "\n",
    "# Visualization\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].scatter(df['Sweetness'], df['Juiciness'], c=df['Cluster_auto_encoded'], cmap='viridis')\n",
    "axes[0].set_title('Cluster Label Encoded')\n",
    "axes[0].set_xlabel('Sweetness')\n",
    "axes[0].set_ylabel('Juiciness')\n",
    "\n",
    "\n",
    "axes[1].scatter(df_combined['Sweetness'], df_combined['Juiciness'], c=df_combined['Cluster_one_hot_encoded'], cmap='viridis')\n",
    "axes[1].set_title('Clustering with OneHotEncoding')\n",
    "axes[1].set_xlabel('Sweetness')\n",
    "axes[1].set_label('Juiciness')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `sparse=False` in the above code.  Notice what happens if we don't do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [['red'], ['blue'], ['green']]\n",
    "\n",
    "encoder_sparse = OneHotEncoder()\n",
    "sparse_output = encoder_sparse.fit_transform(X)\n",
    "\n",
    "pd.DataFrame(sparse_output,columns=encoder_sparse.get_feature_names_out(['Color']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is is because the OneHotEncoder returns a `sparse` matrix by default, which is a memory efficient format for matrices with lots of `0`s.  This cannot be directly converted to a pandas DataFrame with the desired shape, yielding a mismatch between the number of labels and the size of the DataFrame.\n",
    "\n",
    "### Applying One-hot encoding to multiple columns\n",
    "\n",
    "Although you can apply one-hot encoding on a column by column basis, it is often the case that you will have to apply it to multiple columns.  One way to do this is to loop through the columns you care about, and apply one-hot encoding to each in turn.  This would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Initialize an empty DataFrame to hold the transformed columns\n",
    "df_transformed = pd.DataFrame()\n",
    "\n",
    "# Loop through each column in the original DataFrame\n",
    "for col in df.columns:\n",
    "    # Check if the column is of object or category type\n",
    "    if df[col].dtype == 'object' or df[col].dtype.name == 'category':\n",
    "        # Initialize the OneHotEncoder\n",
    "        encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "        \n",
    "        # Fit and transform the column and convert it to a DataFrame\n",
    "        one_hot_encoded = encoder.fit_transform(df[[col]])\n",
    "        one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out([col]))\n",
    "        \n",
    "        # Concatenate the one-hot-encoded DataFrame to the transformed DataFrame\n",
    "        df_transformed = pd.concat([df_transformed, one_hot_df], axis=1)\n",
    "    else:\n",
    "        # If the column is not categorical, just copy it to the transformed DataFrame\n",
    "        df_transformed[col] = df[col].copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
