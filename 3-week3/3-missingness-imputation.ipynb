{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data\n",
    "\n",
    "When dealing with missing data, understanding the mechanism behind the missingness can guide the selection of appropriate imputation methods. The three primary types of missing data are:\n",
    "\n",
    "1. **Missing Completely At Random (MCAR)**: The missingness has no relationship with any other variable, observed or unobserved. It's random.\n",
    "\n",
    "   - **Preferred Imputation Methods**: Mean, median, or mode imputation can be considered. More sophisticated techniques like k-Nearest Neighbors (k-NN) imputation can also be used.\n",
    "   \n",
    "   - **Notes**: Because the data is missing randomly, simpler methods often perform adequately.\n",
    "\n",
    "   - **Example**: Suppose you're collecting data on the heights and weights of a class of students. If you lost some pages of your data collection notebook and that loss is unrelated to the actual data points themselves, then the missing data can be considered MCAR.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic data for height and weight of students\n",
    "np.random.seed(0)\n",
    "n_students = 100\n",
    "height = np.random.normal(160, 10, n_students)  # Average height is 160 cm with a standard deviation of 10\n",
    "\n",
    "weight = 40 + height / 8 # simulate correlation between height and weight\n",
    "noise = np.random.normal(0, 10, n_students)\n",
    "weight+=noise   \n",
    "\n",
    "# Randomly introduce missing data (MCAR)\n",
    "missing_ratio = 0.2  # 20% of the data will be missing\n",
    "missing_indices = np.random.choice(n_students, int(n_students * missing_ratio), replace=False)\n",
    "\n",
    "# Make copies of original data for manipulation\n",
    "height_missing = height.copy()\n",
    "weight_missing = weight.copy()\n",
    "\n",
    "# Introduce missing data\n",
    "height_missing[missing_indices] = np.nan\n",
    "weight_missing[missing_indices] = np.nan\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot of original data\n",
    "axes[0].scatter(height, weight, c='blue', label='Original Data')\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Height (cm)')\n",
    "axes[0].set_ylabel('Weight (kg)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter plot after introducing missing data\n",
    "axes[1].scatter(height_missing, weight_missing, c='orange', label='Data with Missing Values (MCAR)')\n",
    "axes[1].scatter(height[missing_indices], weight[missing_indices], c='blue', label='Omitted Data Points', alpha=0.2)\n",
    "axes[1].set_title('Data with Missing Values (MCAR)')\n",
    "axes[1].set_xlabel('Height (cm)')\n",
    "axes[1].set_ylabel('Weight (kg)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Missing At Random (MAR)**: The probability of missingness may be related to observed data but not the missing data.\n",
    "\n",
    "   - **Preferred Imputation Methods**: Regression imputation, multiple imputation, and expectation-maximization are often used for MAR data.\n",
    "   \n",
    "   - **Notes**: These methods make use of other observed variables to predict and impute the missing variable.\n",
    "\n",
    "   - **Example**: Suppose you're conducting a survey on income and age. Younger people might be less likely to disclose their income, but within the \"young people\" group, the chance of income being missing is random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data for income and age\n",
    "np.random.seed(0)\n",
    "n_people = 100\n",
    "age = np.random.uniform(20, 60, n_people)  # Age ranges from 20 to 60\n",
    "income = 30000 + age * 700  # Simulate a positive correlation between age and income\n",
    "\n",
    "# Add some noise to the income\n",
    "noise = np.random.normal(0, 5000, n_people)\n",
    "income += noise\n",
    "\n",
    "# Introduce missing data (MAR) for younger people\n",
    "# Assume people aged less than 40 are less likely to disclose income\n",
    "missing_ratio_young = 0.4  # 40% of young people's income will be missing\n",
    "young_indices = np.where(age < 40)[0]\n",
    "missing_indices_young = np.random.choice(young_indices, int(len(young_indices) * missing_ratio_young), replace=False)\n",
    "\n",
    "# Make copies of original data for manipulation\n",
    "income_missing = income.copy()\n",
    "\n",
    "# Introduce missing data\n",
    "income_missing[missing_indices_young] = np.nan\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot of original data\n",
    "axes[0].scatter(age, income, c='blue', label='Original Data')\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Income ($)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter plot after introducing missing data\n",
    "axes[1].scatter(age, income_missing, c='orange', label='Data with Missing Values (MAR)')\n",
    "axes[1].scatter(age[missing_indices_young], income[missing_indices_young], c='blue', label='Omitted Data Points', alpha=0.2)\n",
    "axes[1].set_title('Data with Missing Values (MAR)')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('Income ($)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. **Missing Not At Random (MNAR)**: The probability of missingness is related to the missing data itself or a combination of missing and observed data.\n",
    "\n",
    "   - **Preferred Imputation Methods**: More complex methods such as multiple imputation using chained equations or advanced model-based methods can be used. In some cases, sensitivity analyses are performed.\n",
    "  \n",
    "   - **Notes**: Handling MNAR is particularly challenging because you have to model the missingness mechanism itself. This often involves making unverifiable assumptions.\n",
    "\n",
    "   - **Example**: In a medical study, patients with severe symptoms are less likely to return for follow-up. Here, the missingness of the data (follow-up results) is directly related to the outcome (severity of symptoms).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data for symptom severity and follow-up results\n",
    "np.random.seed(0)\n",
    "n_patients = 100\n",
    "severity = np.random.uniform(1, 10, n_patients)  # Severity ranges from 1 to 10\n",
    "follow_up = 30 - severity * 1.5  # Simulate a negative correlation between severity and follow-up results\n",
    "\n",
    "# Add some noise to the follow-up results\n",
    "noise = np.random.normal(0, 3, n_patients)\n",
    "follow_up += noise\n",
    "\n",
    "# Introduce missing data (MNAR) for patients with severe symptoms\n",
    "# Assume patients with severity > 7 are less likely to have follow-up\n",
    "missing_ratio_severe = 0.6  # 60% of severe patients' follow-up will be missing\n",
    "severe_indices = np.where(severity > 7)[0]\n",
    "missing_indices_severe = np.random.choice(severe_indices, int(len(severe_indices) * missing_ratio_severe), replace=False)\n",
    "\n",
    "# Make copies of original data for manipulation\n",
    "follow_up_missing = follow_up.copy()\n",
    "\n",
    "# Introduce missing data\n",
    "follow_up_missing[missing_indices_severe] = np.nan\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot of original data\n",
    "axes[0].scatter(severity, follow_up, c='green', label='Original Data')\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Symptom Severity')\n",
    "axes[0].set_ylabel('Follow-Up Results')\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter plot after introducing missing data\n",
    "axes[1].scatter(severity, follow_up_missing, c='purple', label='Data with Missing Values (MNAR)')\n",
    "axes[1].scatter(severity[missing_indices_severe], follow_up[missing_indices_severe], c='green', label='Omitted Data Points', alpha=0.1)\n",
    "axes[1].set_title('Data with Missing Values (MNAR)')\n",
    "axes[1].set_xlabel('Symptom Severity')\n",
    "axes[1].set_ylabel('Follow-Up Results')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Imputation Methods:\n",
    "\n",
    "1. **Simple Imputation**: Mean, median, or mode imputation is quick and easy but can reduce the variance of the imputed variables and underestimate errors.  Note that this method only uses information from the column in which data is missing.\n",
    "\n",
    "2. **k-Nearest Neighbors (k-NN) Imputation**: This is a more advanced form of imputation that can be used for MCAR or MAR types of missingness. **Important** The KNN approach is based on the \"K Nearest Neighbors\" ML approach, which we'll talk about later.  However, it's important to know that it uses information from other columns in order to identify the most similar rows, and then derives the missing value from that.\n",
    "\n",
    "3. **Multiple Imputation**: This technique is more robust and accounts for the uncertainty of missing values. It is often used for MAR and MNAR types of missingness.\n",
    "\n",
    "4. **Model-based Imputation**: Methods like regression imputation or using machine learning models to predict missing values can be considered, especially when the data is MAR or MNAR.\n",
    "\n",
    "5. **Sensitivity Analysis**: When you have MNAR data, it's often good to perform a sensitivity analysis to understand how your results might change under different assumptions about the missingness mechanism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Imputation by Chained Equations (MICE)\n",
    "\n",
    "Multiple Imputation by Chained Equations (MICE), also known as Fully Conditional Specification (FCS), is a sophisticated method to handle missing data. It is especially useful when you have multiple variables with missing values and those missing values may be Missing At Random (MAR) or even Missing Not At Random (MNAR) to some extent.\n",
    "\n",
    "#### How MICE Works:\n",
    "\n",
    "1. **Initialization**: Start by filling in the missing values with a simple imputation method like mean imputation.\n",
    "  \n",
    "2. **Iterative Imputation**: \n",
    "    - For each variable with missing data:\n",
    "        1. Set its missing values back to missing.\n",
    "        2. Model this variable using the other variables.\n",
    "        3. Use this model to impute the missing values.\n",
    "    - Repeat this process for several iterations to make the imputations more robust.\n",
    "\n",
    "3. **Multiple Datasets**: This process is repeated to create multiple datasets, and analyses are performed on each. \n",
    "\n",
    "4. **Pooling**: Results from these multiple analyses are pooled together to get final estimates.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "\n",
    "<img src=\"./assets/MICE.png\" width=\"500\"/>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Key Features:\n",
    "\n",
    "- **Chained Equations**: Each variable with missing data gets its own imputation model. This allows for different types of variables (e.g., continuous, ordinal, nominal).\n",
    "  \n",
    "- **Uncertainty**: By creating multiple datasets, MICE captures the uncertainty of missing values, thus providing a more accurate estimate of standard errors.\n",
    "\n",
    "- **Flexibility**: You can specify different imputation models for different variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Imputation Methods\n",
    "\n",
    "It's informative to examine the impact of different imputation methods on distributions of data.  Note that this does not necessarily correlate with the performance of an ML algorithm!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "# Create synthetic dataset with specific distributions\n",
    "np.random.seed(0)\n",
    "n = 200  # Number of samples\n",
    "X1 = np.random.normal(5, 2, n)  # Normal distribution\n",
    "X2 = np.random.exponential(1, n)  # Exponential distribution\n",
    "X3 = np.random.uniform(0, 10, n)  # Uniform distribution\n",
    "\n",
    "X_complete = np.column_stack((X1, X2, X3))\n",
    "\n",
    "# Introduce some missing values\n",
    "X_missing = np.copy(X_complete)\n",
    "X_missing[np.random.randint(0, n, 20), 0] = np.nan\n",
    "X_missing[np.random.randint(0, n, 20), 1] = np.nan\n",
    "X_missing[np.random.randint(0, n, 20), 2] = np.nan\n",
    "\n",
    "# Mean Imputation as an initial step\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X_initial_imputed = imp.fit_transform(X_missing)\n",
    "\n",
    "# MICE Imputation\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "X_mice_imputed = imp.fit_transform(X_missing)\n",
    "\n",
    "# KNN Imputation\n",
    "imp = KNNImputer()\n",
    "X_knn_imputed = imp.fit_transform(X_missing)\n",
    "\n",
    "# Random Forest Imputation\n",
    "# Note - there is no built in method for RandomForest imputation, so we manually use a \n",
    "# RandomForestRegressor here to predict missing elements based on other elements.\n",
    "X_rf_imputed = X_initial_imputed.copy()\n",
    "for i in range(X_missing.shape[1]):\n",
    "    missing_idx = np.isnan(X_missing[:, i])\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    rf.fit(X_initial_imputed[~missing_idx, :][:, np.arange(X_missing.shape[1]) != i], X_initial_imputed[~missing_idx, i])\n",
    "    X_rf_imputed[missing_idx, i] = rf.predict(X_initial_imputed[missing_idx, :][:, np.arange(X_missing.shape[1]) != i])\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(6, 3, figsize=(20, 20))\n",
    "\n",
    "# Titles\n",
    "titles = ['Normal Feature', 'Exponential Feature', 'Uniform Feature']\n",
    "imputation_methods = ['Original Complete', 'With Missing Values', 'Mean Imputed', 'MICE Imputed','Random Forest Imputed', 'KNN Imputed']\n",
    "\n",
    "# Loop to plot histograms\n",
    "for j, dataset in enumerate([X_complete, X_missing, X_initial_imputed, X_mice_imputed, X_rf_imputed, X_knn_imputed]):\n",
    "    for i in range(3):\n",
    "        axes[j, i].hist(dataset[:, i][~np.isnan(dataset[:, i])], bins=20, edgecolor='black')\n",
    "        axes[j, i].set_title(f\"{titles[i]} - {imputation_methods[j]}\")\n",
    "        axes[j, i].set_xlabel('Value')\n",
    "        axes[j, i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Making scikit learn and pandas work together\n",
    "\n",
    "The interaction between pandas and scikit-learn can be a bit tricky due to the difference in data structures they expect. Let's take an example using `SimpleImputer` and a single column in a pandas DataFrame to highlight some of these issues.\n",
    "\n",
    "#### What NOT to Do\n",
    "\n",
    "One common mistake is to directly use pandas Series without reshaping it, resulting in an array with a single dimension. This can sometimes cause issues with scikit-learn estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4, 5]})\n",
    "\n",
    "# Initialize SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# This would NOT work as expected, raises an error or gives unwanted results\n",
    "df['A'] = imputer.fit_transform(df['A'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The issue arises because `df['A']` is a pandas Series, and passing it to `fit_transform` without reshaping might not work as expected.\n",
    "\n",
    "#### What to Do\n",
    "\n",
    "The correct approach is to ensure that you are working with a 2D array-like structure, which is what scikit-learn's `SimpleImputer` expects. One way to do this is to use double square brackets when slicing the DataFrame to keep it as a DataFrame with one column.\n",
    "\n",
    "Here's how to do it correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would work as expected, returns a DataFrame\n",
    "correct_df = df[['A']].copy()\n",
    "\n",
    "# Impute missing values\n",
    "correct_df_imputed = imputer.fit_transform(correct_df)\n",
    "\n",
    "# Replace the original column with the imputed one\n",
    "df['A'] = correct_df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Or, you could also reshape the Series to make it into a 2D array explicitly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would work as well, returns a 2D numpy array\n",
    "correct_series = df['A'].values.reshape(-1, 1)\n",
    "\n",
    "# Impute missing values\n",
    "correct_series_imputed = imputer.fit_transform(correct_series)\n",
    "\n",
    "# Replace the original column with the imputed one\n",
    "df['A'] = correct_series_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In both of these correct approaches, we're ensuring that the data passed to `SimpleImputer` is a 2D array-like structure, thus meeting scikit-learn's expectations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
