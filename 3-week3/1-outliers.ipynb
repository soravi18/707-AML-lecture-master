{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "\n",
    "Outliers can have a significant impact on the results. Here’s a quick guide to detecting and dealing with outliers:\n",
    "\n",
    "### **Detecting Outliers**\n",
    "#### a. Visual Inspection:\n",
    "   - **Box Plots:** A box plot represents the distribution of the data and its central tendency. Points that are located outside the \"whiskers\" of the box plot are typically considered as outliers.\n",
    "   - **Scatter Plots:** Useful to see the spread of data and identify potential outliers in bivariate analysis.\n",
    "   - **Histograms:** Helps to identify outliers in univariate analysis by visualizing the data distribution.\n",
    "\n",
    "#### b. Statistical Methods:\n",
    "   - **Z-Score:** A Z-score represents the number of standard deviations a data point is from the mean. A high absolute value of Z-score (typically above 3) indicates that the data point is likely an outlier.\n",
    "   - **IQR Method:** Outliers can be identified by finding values that lie below Q1 - 1.5*IQR or above Q3 + 1.5*IQR, where Q1 and Q3 are the first and third quartiles, respectively, and IQR is the interquartile range (Q3 - Q1).\n",
    "\n",
    "### **Handling Outliers**\n",
    "#### a. Removal:\n",
    "   - **Truncation:** Simply remove outlier values.\n",
    "   - **Winsorizing:** Cap the outlier values to a certain threshold.\n",
    "\n",
    "#### b. Transformation:\n",
    "   - **Log Transformation:** Reduces the impact of outliers but requires all values to be positive.\n",
    "   - **Box-Cox Transformation:** Generalizes the power transformation and can stabilize variance and make the data more normal distribution-like.\n",
    "\n",
    "#### c. Imputation:\n",
    "   - Replace outlier values with statistical measures such as mean, median, or mode.\n",
    "\n",
    "#### d. Robust Methods:\n",
    "   - Use models and methods that are robust to outliers, such as robust regression methods, tree-based models, or ensemble methods.\n",
    "\n",
    "### Example\n",
    "Here’s an example of detecting and handling outliers using Python and the `scipy.stats` module for Z-score and the `numpy` library for IQR method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# More complex example data\n",
    "np.random.seed(0)\n",
    "data = {'Value': np.random.normal(25, 5, 100).tolist() + [100, 105, 110, 150, 200]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Original DataFrame\n",
    "original_df = df.copy()\n",
    "\n",
    "# Detecting outliers using Z-score\n",
    "df['Z_Score'] = zscore(df['Value'])\n",
    "outliers_z_score = df[np.abs(df['Z_Score']) > 3]\n",
    "\n",
    "# Detecting outliers using IQR method\n",
    "Q1 = df['Value'].quantile(0.25)\n",
    "Q3 = df['Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = df[(df['Value'] < (Q1 - 1.5 * IQR)) | (df['Value'] > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "# Handling outliers: Removal\n",
    "df_no_outliers_z = df[np.abs(df['Z_Score']) <= 3]\n",
    "df_no_outliers_iqr = df[(df['Value'] >= (Q1 - 1.5 * IQR)) & (df['Value'] <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "# Handling outliers: Imputation (Replace with Median)\n",
    "df_imputed = df.copy()\n",
    "df_imputed['Value'].where(~df.index.isin(outliers_z_score.index), df['Value'].median(), inplace=True)\n",
    "\n",
    "# Create Subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axs[0, 0].boxplot(original_df['Value'])\n",
    "axs[0, 0].set_title('Original Data')\n",
    "\n",
    "axs[0, 1].boxplot(df_no_outliers_z['Value'])\n",
    "axs[0, 1].set_title('Data after Outlier Removal (Z-score)')\n",
    "\n",
    "axs[1, 0].boxplot(df_no_outliers_iqr['Value'])\n",
    "axs[1, 0].set_title('Data after Outlier Removal (IQR)')\n",
    "\n",
    "\n",
    "axs[1, 1].boxplot(df_imputed['Value'])\n",
    "axs[1, 1].set_title('Data after Outlier Imputation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that outliers aren't merely statistical anomalies - they are also values that are unrealistic!  So it's often a good idea to inspect your data carefully when you are trying to remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling Outliers in sklearn**\n",
    "\n",
    "Scikit-Learn provides several tools for handling outliers. Note that most of these tools follow the estimation API, meaning they provide `fit` and `predict` methods.\n",
    "\n",
    "### 1. Robust Scalers\n",
    "- **RobustScaler**: `RobustScaler` scales features using statistics that are robust to outliers by removing the median and scaling data according to the interquartile range.  Note that unlike other methods,`RobustScaler` is a transformer, and is really a tool for scaling data rather than removing outliers. \n",
    "- **Use Case**: Ideal for datasets with outliers, where you want to scale features without being influenced by them.\n",
    "\n",
    "### 2. EllipticEnvelope\n",
    "- **What It Does**: Fits a robust covariance estimate to the data, thus identifying the data points that are statistical outliers in a Gaussian distributed dataset.\n",
    "- **Use Case**: Useful in detecting outliers when the data is assumed to have a Gaussian distribution.\n",
    "\n",
    "### 3. Isolation Forest\n",
    "- **What It Does**: This algorithm isolates anomalies instead of profiling normal data points. It works well for high-dimensional datasets.\n",
    "- **Use Case**: Effective for outlier detection, particularly for datasets where the number of outliers is expected to be low.\n",
    "\n",
    "### 4. Local Outlier Factor (LOF)\n",
    "- **What It Does**: Measures the local deviation of a given data point with respect to its neighbors. It considers outliers as those points that have a substantially lower density than their neighbors.\n",
    "- **Use Case**: Particularly useful for anomaly detection in datasets where the density of the data is not uniform.\n",
    "\n",
    "### 5. One-Class SVM\n",
    "- **What It Does**: This unsupervised algorithm learns a decision function for novelty detection, identifying outliers as data points that do not conform to the learned region.\n",
    "- **Use Case**: Suitable for outlier detection in feature spaces where the data is not too noisy.\n",
    "\n",
    "### Example of Using Isolation Forest:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "outliers = iso.fit_predict(X)\n",
    "\n",
    "# outliers will be an array of -1 for outliers and 1 for inliers\n",
    "```\n",
    "\n",
    "### Important Points:\n",
    "- **Parameter Tuning**: These methods often have parameters like `contamination` that specify the proportion of outliers in the data, which might need tuning based on the specific dataset.\n",
    "- **Unsupervised Methods**: Most outlier detection methods in Scikit-Learn are unsupervised, meaning they don't require labeled data indicating which points are outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Outlier removal in pipelines**\n",
    "\n",
    "In Scikit-Learn, there isn't a direct, built-in way to remove outliers using tools like `IsolationForest` within a pipeline. The standard pipeline components in Scikit-Learn, such as transformers and estimators, are designed to transform or predict, but not to filter or remove data points.\n",
    "\n",
    "However, you can create a custom transformer that incorporates outlier detection and data removal. This transformer can then be integrated into a Scikit-Learn pipeline. \n",
    "\n",
    "Here's an example of how you might implement such a custom transformer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_estimators=100, contamination='auto'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.contamination = contamination\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.isolation_forest = IsolationForest(n_estimators=self.n_estimators, \n",
    "                                                contamination=self.contamination)\n",
    "        self.isolation_forest.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Predict outliers (-1 for outliers, 1 for inliers)\n",
    "        outliers = self.isolation_forest.predict(X)\n",
    "        # Filter out the outliers\n",
    "        return X[outliers == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage in a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('remove_outliers', OutlierRemover()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Important Points:\n",
    "- **Custom Transformer**: `OutlierRemover` is a custom transformer that uses `IsolationForest` for outlier detection. It filters out outliers in the `transform` method.\n",
    "- **Pipeline Integration**: This transformer can be used as a step in a Scikit-Learn Pipeline, ensuring that outlier removal is appropriately applied during cross-validation or other model evaluation methods.\n",
    "- **Caution**: Be cautious with automatically removing outliers, as they might be genuine data points that are important for the model to learn. Always inspect and understand your data before deciding to remove outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
