{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once data has been loaded and some initial cleaning is done, it is often necessary to perform several addition rounds of data preparation.  This can include scaling data, removing outliers, re-encoding variables, and imputing missing data.  We'll cover these aspects below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "Many machine learning algorithms, such as k-means clustering and support vector machines, use distance metrics like Euclidean distance to compare points in the feature space. Features with large numeric ranges can dominate the distance computation, thereby affecting the algorithm's performance. Similarly, optimization algorithms like gradient descent converge more quickly when features are on similar scales.\n",
    "\n",
    "### Working Example - Impact of Scaling on K-Means Clustering\n",
    "\n",
    "Let's consider a simple synthetic dataset with two features `X1` and `X2`, where `X1` has values ranging between 0 and 10, but are separated into two groups along the axis. `X2` ranges from 0 to 1000, and so dominates the distance function. We'll cluster the data using k-means before and after scaling to see the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X1a = np.random.uniform(0, 4, 50)\n",
    "X1b = np.random.uniform(6, 10, 50)\n",
    "X1 = np.concatenate([X1a,X1b])\n",
    "X2 = np.random.uniform(0, 1000, 100)\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Cluster without scaling\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('K-Means Clustering without Scaling')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()\n",
    "\n",
    "# Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Cluster after scaling\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('K-Means Clustering with Scaling')\n",
    "plt.xlabel('X1 (scaled)')\n",
    "plt.ylabel('X2 (scaled)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How to scale\n",
    "\n",
    "Not all distributions are created equal!  It is important to examine your distributions before scaling parameters, or else your scaling efforts might not yield any improvements.\n",
    "\n",
    "Two common types of distributions are normal and power-law (heavy-tailed) distributions.  These are easy to recognize by plotting histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 1000\n",
    "\n",
    "# Power-law (exponential) distribution\n",
    "power_law_data = np.random.exponential(scale=10, size=n_samples)\n",
    "\n",
    "# Scaled\n",
    "scaled_power_law = np.log1p(power_law_data)\n",
    "\n",
    "# Normal distribution\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "normal_data_2D = normal_data.reshape(-1, 1)\n",
    "\n",
    "# Apply scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_normal_data_2D = scaler.fit_transform(normal_data_2D)\n",
    "\n",
    "# Convert back to 1D array\n",
    "scaled_normal_data = scaled_normal_data_2D.ravel()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot power-law distribution\n",
    "axes[0,0].hist(power_law_data, bins=30, color='blue', edgecolor='black')\n",
    "axes[0,0].set_title(\"Power-law (Exponential) Distribution\")\n",
    "axes[0,0].set_xlabel(\"Value\")\n",
    "axes[0,0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot scaled power-law distribution\n",
    "axes[1,0].hist(scaled_power_law, bins=30, color='blue', edgecolor='black')\n",
    "axes[1,0].set_title(\"Scaled Exponential Distribution\")\n",
    "axes[1,0].set_xlabel(\"Value\")\n",
    "axes[1,0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot normal distribution\n",
    "axes[0,1].hist(normal_data, bins=30, color='green', edgecolor='black')\n",
    "axes[0,1].set_title(\"Normal Distribution\")\n",
    "axes[0,1].set_xlabel(\"Value\")\n",
    "axes[0,1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Plot scaled normal distribution\n",
    "axes[1,1].hist(scaled_normal_data, bins=30, color='green', edgecolor='black')\n",
    "axes[1,1].set_title(\"Scaled Normal Distribution\")\n",
    "axes[1,1].set_xlabel(\"Value\")\n",
    "axes[1,1].set_ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **Z-scaling**: Use it when the feature roughly follows a normal distribution or when you don't have information about the distribution. It transforms the data into a distribution with a mean of 0 and a standard deviation of 1.\n",
    "  \n",
    "- **Log-Scaling**: It is useful for features that follow a power-law distribution. In these cases, log-scaling can help equalize the ranges and variances across features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling in sklearn\n",
    "\n",
    "Scikit-Learn provides several tools for scaling data, each with its own use cases and characteristics. All scaling tools follow the transformer API, meaning that they offer `fit`, `transform`, and `fit_transform` methods. Here's a brief overview to the most commonly used scaling methods available in Scikit-Learn:\n",
    "\n",
    "### 1. StandardScaler\n",
    "- **What It Does**: Standardizes features by removing the mean and scaling to unit variance. This results in a distribution with a standard deviation of 1.\n",
    "- **Use Case**: When you need features to have a mean of 0 and a standard deviation of 1. Ideal for algorithms that assume features are normally distributed.\n",
    "\n",
    "### 2. MinMaxScaler\n",
    "- **What It Does**: Scales each feature to a given range, typically between 0 and 1. The transformation is given by: `(X - X.min()) / (X.max() - X.min())`.\n",
    "- **Use Case**: When you need to scale features to a specific range. Itâ€™s sensitive to outliers, so if outliers are present, consider using RobustScaler.\n",
    "\n",
    "### 3. RobustScaler\n",
    "- **What It Does**: Scales features using statistics that are robust to outliers. It removes the median and scales the data according to the interquartile range.\n",
    "- **Use Case**: When your data contains outliers and you want to reduce their influence. However, note that outliers themselves are not removed.\n",
    "\n",
    "### 4. Normalizer\n",
    "- **What It Does**: Normalizes samples (not features) individually to unit norm (vector length).\n",
    "- **Use Case**: This is useful for sparse datasets (lots of zeros) with features of varying scales when using algorithms that are sensitive to the size of feature vectors, like k-NN or SVMs.\n",
    "\n",
    "### Example of Using StandardScaler:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Important Points:\n",
    "- **Fit vs Transform**: The `fit` method computes the necessary statistics (mean, standard deviation, etc.), and `transform` applies the scaling. `fit_transform` does both steps together.\n",
    "- **Scaling Train and Test Data**: Always fit the scaler on your training data and then transform both your training and test data to avoid data leakage.\n",
    "- **Inverse Transformation**: Most scalers in Scikit-Learn offer an `inverse_transform` method to revert the data back to its original scale, which can be useful for interpretation or further processing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
