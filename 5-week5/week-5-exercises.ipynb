{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Given the following data:\n",
    "\n",
    "| Tid | Refund | Marital Status | Taxable Income (K) | Cheat |\n",
    "|-----|--------|----------------|--------------------|-------|\n",
    "| 1   | Yes    | Single         | 125                | No    |\n",
    "| 2   | No     | Married        | 100                | No    |\n",
    "| 3   | No     | Single         | 70                 | No    |\n",
    "| 4   | Yes    | Married        | 120                | No    |\n",
    "| 5   | No     | Divorced       | 95                 | Yes   |\n",
    "| 6   | No     | Married        | 60                 | No    |\n",
    "| 7   | Yes    | Divorced       | 220                | No    |\n",
    "| 8   | No     | Single         | 85                 | Yes   |\n",
    "| 9   | No     | Married        | 75                 | No    |\n",
    "| 10  | No     | Single         | 90                 | Yes   |\n",
    "\n",
    "What is the best first split (using Gini - note, for the continuous feature, check quartile boundaries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Build a decision tree to fit the [federalist papers](https://www.kaggle.com/datasets/tobyanderson/federalist-papers_) data, available in the data directory (click on the link to find out more information about this data). Note that you should restrict your analysis to papers by Hamilton or Madison.  Plot your training and test scores to pick a value for ccp_alpha. What did you pick?  Run your trained classifier on the \"disputed\" papers.  What does your model tell you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Build a voting classifier for the federalist papers, using all of the non-ensemble methods you've been exposed to in this class thus far (i.e., KNN, SVM, logistic regression, naive bayes, SGDClassifier, decision tree).\n",
    "\n",
    "1) Compare this to a RandomForest classifier.  Which works the best?\n",
    "2) Compare this to a GradientBoosting classifier.  Which works the best?\n",
    "3) Add the RandomForest and GradientBoosting classifiers to your voting classifier.  Does you performance improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "When does it make sense to use a Bagging Classifier?  In the following, explore different data parameters to develop your intuition for which classifier makes sense in which situation. \n",
    "\n",
    "1. Gradually increase the noise in the data (using the noise parameter).  How do the different classifiers perform.  Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "y_train_pred_tree = tree.predict(X_train)\n",
    "print(\"Decision Tree Accuracy (train):\", accuracy_score(y_train, y_train_pred_tree))\n",
    "print(\"Decision Tree Accuracy (test):\", accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "y_train_pred_log = log_reg.predict(X_train)\n",
    "print(\"Logistic Regression Accuracy (train):\", accuracy_score(y_train, y_train_pred_log))\n",
    "print(\"Logistic Regression Accuracy (test):\", accuracy_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Now, do the same thing in the following.  What do you notice. How do you explain your observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "max_samples = 100\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "bag_tree = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, random_state=42,max_samples=max_samples)\n",
    "bag_tree.fit(X_train, y_train)\n",
    "y_pred_bag_train = bag_tree.predict(X_train)\n",
    "y_pred_bag = bag_tree.predict(X_test)\n",
    "print(\"Bagging Decision Tree Accuracy (train):\", accuracy_score(y_train, y_pred_bag_train))\n",
    "print(\"Bagging Decision Tree Accuracy (test):\", accuracy_score(y_test, y_pred_bag))\n",
    "\n",
    "\n",
    "bag_log = BaggingClassifier(LogisticRegression(), n_estimators=500, random_state=42,max_samples=max_samples)\n",
    "bag_log.fit(X_train, y_train)\n",
    "y_pred_bag_log_train = bag_log.predict(X_train)\n",
    "y_pred_bag_log = bag_log.predict(X_test)\n",
    "print(\"Bagging Logistic Regression Accuracy (train):\", accuracy_score(y_train, y_pred_bag_log_train))\n",
    "print(\"Bagging Logistic Regression Accuracy (test):\", accuracy_score(y_test, y_pred_bag_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now go back and start increasing the `max_samples` parameter.  How do things change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "The \"wine\" dataset contains data about the chemical makeup of different varieties of wine and critics scores.  Use XGBoost to build a classifier for this data.  Manually tune the hyperparameters of the XGBoost model to try to achieve better accuracy on the test set than the baseline model. Some hyperparameters to consider tweaking:\n",
    "   - `learning_rate`\n",
    "   - `max_depth`\n",
    "   - `n_estimators`\n",
    "   - `gamma`\n",
    "   - `subsample`\n",
    "   - `colsample_bytree`\n",
    "\n",
    "See [the online docs](https://xgboost.readthedocs.io/en/stable/parameter.html) for more info.\n",
    "\n",
    "After tuning, use the `plot_importance` function again to see if feature importances have changed after tuning.\n",
    "\n",
    "\n",
    "1. How did hyperparameter tuning affect the model's accuracy? Which hyperparameters seemed to have the most influence?\n",
    "2. Did feature importances change after tuning? If so, why might that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you don't have XGBoost installed\n",
    "%pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "data = load_wine()\n",
    "\n",
    "# We'll use a data frame to make sure we get real feature names out\n",
    "X = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = xgb.XGBClassifier(objective='multi:softprob', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "baseline_accuracy = clf.score(X_test, y_test)\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "\n",
    "plot_importance(clf)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
