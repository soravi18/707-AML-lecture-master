{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "\n",
    "Naive Bayes (NB) is one of the simplest yet surprisingly powerful algorithms in the field of Machine Learning. It is rooted in probability theory and is fundamentally a classification algorithm, meaning it assigns objects (or data points) to predefined categories or classes based on their features.\n",
    "\n",
    "NB is computationally efficient and easy to implement, which makes it ideal for quick prototyping and handling large datasets. It found it's first widespread application in email spam filtering and content recommendation systems. It remains a popular choice for text classification tasks, including sentiment analysis, due to its efficacy with high-dimensional data, and in real-time comment moderation on various online platforms. It is also widely used in academic research, such as in medicine and biology, where it is used to identify patterns or make predictions based on genomic or clinical data.\n",
    "\n",
    "### **Why Naive Bayes:**\n",
    "Despite its simplicity and ‘naivety’, Naive Bayes has several advantages that make it a good choice in specific scenarios.\n",
    "\n",
    "- **Simplicity & Efficiency:**\n",
    "    - It performs well with high-dimensional data, where many features are involved, like text data.\n",
    "\n",
    "- **Probabilistic Nature:**\n",
    "    - It provides probabilities for the outcomes, allowing for a graded measure of classification confidence, which is not always the case with decision trees or SVMs.\n",
    "    - This probabilistic nature allows for the incorporation of uncertainty in the model, which can be crucial in certain domains like healthcare.\n",
    "\n",
    "- **Robust to Irrelevant Features:**\n",
    "    - Due to its conditional independence assumption, it can be robust to irrelevant features.\n",
    "    - In contrast, decision trees can create overly complex trees and overfit to noise in the presence of irrelevant features.\n",
    "\n",
    "- **Performs Well with Less Data:**\n",
    "    - Unlike more complex models like neural networks, it can perform reasonably well with a smaller amount of training data.\n",
    "    - This makes it suitable for scenarios where collecting vast amounts of labeled data is not feasible.\n",
    "\n",
    "### **Probability Theory Introduction:**\n",
    "\n",
    "To lay the groundwork for Naive Bayes, it is important to have a basic understand of probability theory.  We'll cover some of the rudiments here.\n",
    "\n",
    "#### **Basic Probability** ####\n",
    "\n",
    "- An **event** is an outcome or a combination of outcomes from a random experiment. It's a set containing all the outcomes of interest. For example, when tossing a coin (an experiment), a set of two events are possible: Getting a Head (H) or a Tail (T).\n",
    "   \n",
    "- **Probability** quantifies the likelihood that a given event will occur. The probability of an event $A$ occurring is denoted by $P(A)$. For example, the probability of getting a Head (H) in a fair coin toss is $\\frac{1}{2}$ or 0.5.  In general, if $N$ events in an experiment have the same likelihood, the probability of any one of them occurring is $\\frac{1}{N}$.\n",
    "\n",
    "![Probability image](./assets/probability_theory1.png)\n",
    "\n",
    "- **Joint Probability** is the probability of two multiple occurring together in a given experiment. The joint probability of events $A$ and $B$ is denoted by $P(A \\cap B)$, $P(A \\text{ and } B)$, or sometimes $P(A,B)$.  The order of $A$ and $B$ do not matter - that is, $P(A \\cap B) = P(B \\cap A)$.\n",
    "\n",
    "  When flipping a coin in an experiment with one flip, the joint probability of getting a heads and a tails is 0, as only one event can occur in the experiment and these are mutually exclusive events.\n",
    "\n",
    "![Joint probability image](assets/joint_probability_diagram.webp)\n",
    "\n",
    "- Two events are said **independent** if the occurrence of one event does not affect the occurrence of the other. If $A$ and $B$ are independent, then $P(A \\cap B) = P(A) \\cdot P(B)$. For example, if an experiment covers two successive coin tosses, getting a Head (H) in the first toss is independent of getting a Head (H) in the second toss. Therefore $P(H_{1} \\cap H_{2}) = P(H_{1}) \\cdot P(H_{2}) = .5 \\cdot .5 = .25$\n",
    "\n",
    "#### **Conditional Probability**\n",
    "\n",
    "When events are not independent though, we need to calculate their joint probability in a different way.  For example, suppose an experiment involves choosing two cards from a standard deck of 52 cards. If the first card is a queen, what is the probability of choosing another so that the final draw is two queens? Because the two events are no longer independent, we need another way to calculate the overall probability.\n",
    " **Conditional probability** is the probability of an event occurring given that another event has occurred. The probability of event $A$ given event $B$ has occurred is denoted by $P(A|B)$.  Unlike with joint probability, order in notation matters - $P(A|B)$ is not the same as $P(B|A)$.\n",
    "\n",
    "The relationship between joint probability and conditional probably can be expressed as:\n",
    "\n",
    "$$P(A,B) = P(A) \\cdot P(B|A)$$\n",
    "\n",
    "Flipping this around gives us the definition of conditional probability:\n",
    "\n",
    "$$P(B|A) = \\frac{P(A,B)}{P(A)}, \\text{if } P(A) \\neq 0$$\n",
    "\n",
    "Considering the example of drawing two queens, the probability of drawing the first queen $P(Q_1)$ is $\\frac{4}{52} =~.078$.  However, the probability of drawing a second queen $Q_2$ after the first, $P(Q_2|Q_1) = \\frac{3}{51} =~ .59$.  Therefore, the joint probability $P(Q_1,Q_2) = P(Q_1) \\cdot P(Q_2|Q_1) = \\frac{1}{13} \\cdot  \\frac{1}{17} = \\frac{1}{221}$.\n",
    "\n",
    "**Sequential events vs. concurrent evidence**\n",
    "\n",
    "Note that there are two types of conditional probability; that wherein two events are connected in time (e.g., sequential card draws), and that wherein one event serves as evidence of another (e.g., a funny noise from the engine suggests a mechanical failure).  There is no formal distinction between these two kinds of conditional probability, but which one we're working with may have implications for modeling in a given domain. \n",
    "\n",
    "#### **Bayes’ Theorem:**\n",
    "\n",
    "Bayes theorem, named after the statistician and philosopher Reverend Thomas Bayes, builds on the relationship between joint probability and conditional probability described above.  Bayes’ theorem relates the conditional and *marginal* probabilities of random variables. The marginal probability is just the overall probability of an event, disregarding the outcomes of other events - it can be thought of as an aggregate probability given all of the possible ways the event can occur. \n",
    "\n",
    "Bayes theorem provides us with a a way to update our previous beliefs based on new evidence. Given two events A and B, we know that:\n",
    "\n",
    "$$P(A,B) = P(B|A)\\cdot P(A)$$\n",
    "\n",
    "and also that:\n",
    "\n",
    "$$P(B,A) = P(A|B) \\cdot P(B)$$\n",
    "\n",
    "but since $P(A,B) = P(B,A)$,\n",
    "\n",
    "$$P(B|A)\\cdot P(A) = P(A|B) \\cdot P(B)$$\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "The components of Bayes Theorem are sometimes referred to as follows:\n",
    "\n",
    "1. **Posterior Probability, $P(A|B)$:** This is what we are trying to find out: the probability of our hypothesis $A$ being true, given the observed evidence $B$. It’s what we know after we have taken into account the evidence, hence it’s called the posterior, meaning it’s what we know afterward.\n",
    "2. **Likelihood, $P(B|A)$:** The probability of observing the given evidence $$ if our hypothesis $A$ is true. It quantifies how well our evidence supports our hypothesis. How likely are you to see the evidence under your hypothesis? It’s called likelihood because it tells us about the likelihood of seeing our observations.\n",
    "3. **Prior Probability, $P(A)$:** This is our initial belief about the probability of hypothesis $A$ before observing the evidence $B$. It’s based on previous knowledge or experience. It’s our prior belief or knowledge about the hypothesis before seeing the evidence. It’s what we know prior to observing the evidence.\n",
    "4. **Evidence, $P(B)$:**  It represents the probability of observing the evidence $B$. It’s the normalizing constant ensuring that our posterior probabilities sum up to one. It’s what we observe; it’s the data we have, the evidence we see, hence the name.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Rhe marginal probability a woman has breast cancer is 0.8 percent. If a woman has breast cancer, the probability is 90 percent that she will have a positive mammogram. If a  woman does not have breast cancer, the probability is 7 percent that she will still have a positive mammogram. _What is is the probability a woman has breast cancer given a positive mammogram?_\n",
    "\n",
    "We can solve this using Bayes Theorem as follows.  First, consider:\n",
    "\n",
    "- $P(A|B)$ is the **posterior probability** that a woman has breast cancer given that she has a positive mammogram.\n",
    "- $P(B|A)$ is the **likelihood** of a positive mammogram given that a woman has breast cancer, 90% or 0.9.\n",
    "- $P(A)$ is the marginal, or **prior** probability that a woman has breast cancer, 0.8% or 0.008.\n",
    "- $P(B)$ is the overall probability of a positive mammogram; this is the probability of the **evidence**.\n",
    "\n",
    "One useful way to understand this is via a contingency table. Let’s consider 1000 women.\n",
    "\n",
    "- 0.8% of 1000 women have breast cancer: $0.008 \\times 1000 = 8$ women.\n",
    "- 99.2% of 1000 women do not have breast cancer: $0.992 \\times 1000 = 992$ women.\n",
    "\n",
    "So the table starts to look like this:\n",
    "\n",
    "\n",
    "|  | Positive Mammogram | Negative Mammogram | Total |\n",
    "|---|---|---|---|\n",
    "| Cancer  | | | 8 |\n",
    "| No Cancer | | | 992 |\n",
    "\n",
    "Now we can calculate specific probabilities:\n",
    "\n",
    "- Of the 8 women with breast cancer, 90% will have a positive mammogram: $0.9 \\times 8 = 7.2 \\approx 7$ women.\n",
    "- Of the 992 women without breast cancer, 7% will still have a positive mammogram: $0.07 \\times 992 = 69.44 \\approx 69$ women.\n",
    "\n",
    "So, the table becomes:\n",
    "\n",
    "|  | Positive Mammogram | Negative Mammogram | Total |\n",
    "|---|---|---|---|\n",
    "| Cancer  | 7 | 1 | 8 |\n",
    "| No Cancer | 69 | 923 | 992 |\n",
    "| Total | 76 | 924 | 1000 |\n",
    "\n",
    "Applying Bayes theorem, we can find the probability that a woman has breast cancer given she has a positive mammogram:\n",
    "\n",
    "- $P(B|A) = 0.9$ (probability of a positive mammogram given breast cancer)\n",
    "- $P(A) = 0.008$ (marginal probability of breast cancer)\n",
    "- $P(B)$ is the overall probability of a positive mammogram, which is the sum of the probability of a positive mammogram with cancer and without cancer. From our table, we can see this as $\\frac{76}{1000} = 0.076$.\n",
    "\n",
    "Plugging in these values gives us:\n",
    "\n",
    "$$P(A|B) = \\frac{0.9 \\cdot 0.008}{0.076} \\approx 0.0947$$\n",
    "\n",
    "So, the probability that a woman has breast cancer given that she has received a positive mammogram is approximately 9.47%.\n",
    "\n",
    "**Note**\n",
    "\n",
    "Although we've used a contingency table to help visualize the probability calculations here, we can derive the marginal probability that a woman will receive a positive mammogram can be calculated as the sum of the probability of a true positive and the probability of a false positive.\n",
    "\n",
    "Given:\n",
    "- $P(A)$: The probability a woman has breast cancer, which is 0.8% or 0.008.\n",
    "- $P(A')$: The probability a woman does not have breast cancer, which is 0.992.\n",
    "- $P(B|A)$: The probability of a positive mammogram given a woman has breast cancer, which is 0.9.\n",
    "- $P(B|A')$: The probability of a positive mammogram given a woman does not have breast cancer, i.e., the probability of a false positive, which is 0.07.\n",
    "\n",
    "Using the Law of Total Probability, the marginal probability of a positive mammogram, $P(B)$, is given by:\n",
    "\n",
    "$$P(B) = P(B|A) \\cdot P(A) + P(B|A') \\cdot P(A')$$\n",
    "\n",
    "Plugging in the known values:\n",
    "\n",
    "$P(B) = (0.9 \\cdot 0.008) + (0.07 \\cdot 0.992)$\n",
    "$P(B) = 0.0072 + 0.06944$\n",
    "$P(B) = 0.07664$\n",
    "\n",
    "So, the marginal probability that a woman will receive a positive mammogram is approximately $7.664\\%$.\n",
    "\n",
    "**Interpretation**\n",
    "This result is crucial in medical testing and diagnosis, demonstrating that even with a seemingly accurate test, the actual probability of having the disease can be much lower due to the rarity of the condition and the presence of false positives. This example underscores the importance of considering base rates (or prior probabilities) and the probabilities of false positives and false negatives when interpreting diagnostic test results.\n",
    "\n",
    "### **Naive Bayes**\n",
    "\n",
    "When predicting the likelihood of breast cancer, clinicians often consider multiple pieces of evidence or features, such as:\n",
    "1. Family History (Present/Absent)\n",
    "2. Positive Mammogram (Yes/No)\n",
    "3. Alcohol Use (Yes/No)\n",
    "\n",
    "Given these three features, there are $2^{3} = 8$ different feature combinations for each individual, i.e., each individual can be represented in one of 8 different states regarding these features. Furthermore, for each of these states, an individual could either have cancer or not, resulting in a total of $2 \\times 2^{3} = 16$ possible scenarios:\n",
    "\n",
    "| P(Cancer)           | P(No Cancer)         |\n",
    "|---------------------|----------------------|\n",
    "| P(C \\| +M, +F, +A)  | P(¬C \\| +M, +F, +A)  |\n",
    "| P(C \\| +M, +F, -A)  | P(¬C \\| +M, +F, -A)  |\n",
    "| P(C \\| +M, -F, +A)  | P(¬C \\| +M, -F, +A)  |\n",
    "| P(C \\| +M, -F, -A)  | P(¬C \\| +M, -F, -A)  |\n",
    "| P(C \\| -M, +F, +A)  | P(¬C \\| -M, +F, +A)  |\n",
    "| P(C \\| -M, +F, -A)  | P(¬C \\| -M, +F, -A)  |\n",
    "| P(C \\| -M, -F, +A)  | P(¬C \\| -M, -F, +A)  |\n",
    "| P(C \\| -M, -F, -A)  | P(¬C \\| -M, -F, -A)  |\n",
    "\n",
    "\n",
    "If we were to compute the conditional probabilities for cancer presence based on each of these combinations directly, the number of probabilities would grow exponentially with each additional feature. Moreover, acquiring sufficient data for each of these exponentially growing states can be challenging, leading to sparsity issues, which can in turn lead to unreliable probability estimates.\n",
    "\n",
    "This is where the Naive Bayes algorithm comes in as a solution. It simplifies the calculation by assuming that each feature is independent of the other features given the class label, hence the term “Naive”. It means that the presence or absence of a particular feature does not affect the presence or absence of any other feature, as long as we know the class label.\n",
    "\n",
    "#### **Mathematical Framework:**\n",
    "Mathematically, if we denote the class variable as $C$ (Cancer/No Cancer) and the features as $F_1, F_2, F_3$ (Family History, Mammogram Result, Alcohol Use), the Naive Bayes model represents the joint distribution as follows:\n",
    "\n",
    "$$P(C, F_1, F_2, F_3) = P(C) \\cdot P(F_1 | C) \\cdot P(F_2 | C) \\cdot P(F_3 | C)$$\n",
    "\n",
    "That is, we have effectively ignored all of the interactions here, reducing the problem of probability estimation from one where we need $2 \\times 2^n$ probabilities for $n$ features and 2 outcomes, to one in which we only need $2 * n$ probabilities.  \n",
    "\n",
    "Despite its simplicity and the naivety in its assumptions, Naive Bayes can be surprisingly effective in practice, especially when the dimensionality of the data is high, and the data is sparse, which often occurs in text classification, spam filtering, and medical diagnosis.\n",
    "\n",
    "#### **Applying Bayes Rule for Classification**\n",
    "\n",
    "Given a class variable $C$ (e.g., Cancer or No Cancer) and a set of feature variables $F_1, F_2, ..., F_n$ (e.g., Family History, Mammogram Result, Alcohol Use, etc.), the task is to compute the probability of a particular class $C$ given the observed features.According to Bayes’ Rule, the probability of class $C$ given the features is computed as:\n",
    "\n",
    "$$P(C | F_1, F_2, ..., F_n) = \\frac{P(F_1, F_2, ..., F_n | C) \\cdot P(C)}{P(F_1, F_2, ..., F_n)}$$\n",
    "\n",
    "Under the Naive Bayes assumption of conditional independence of features given the class, the likelihood term simplifies as:\n",
    "\n",
    "$$P(F_1, F_2, ..., F_n | C) = P(F_1 | C) \\cdot P(F_2 | C) \\cdot ... \\cdot P(F_n | C)$$\n",
    "\n",
    "Plugging the simplified likelihood back into Bayes' Rule gives us the Naive Bayes formula for computing the posterior probability:\n",
    "\n",
    "$$P(C | F_1, F_2, ..., F_n) = \\frac{P(C) \\cdot \\prod_{i=1}^{n} P(F_i | C)}{P(F_1, F_2, ..., F_n)}$$\n",
    "\n",
    "In a classification task, we typically are interested in finding the class with the highest posterior probability, i.e., we choose the class $C$ that maximizes $P(C | F_1, F_2, ..., F_n)$. Since the denominator is constant for all classes, we can omit it for the purpose of maximizing the expression, and we get:\n",
    "\n",
    "$$\\hat{C} = \\arg\\max_C P(C) \\cdot \\prod_{i=1}^{n} P(F_i | C)$$\n",
    "\n",
    "More concretely, if we were predicting whether a person has cancer ($C$) based on a set of features such as family history ($F_1$), mammogram result ($F_2$), and alcohol use ($F_3$), we would calculate the probability of cancer and no cancer for each combination of feature values and choose the one with the higher probability.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Under the naive Bayes assumption, predict whether someone with a positive mammogram, a family history of cancer, and who uses alcohol has cancer.  As a reminder, here are the steps:\n",
    "\n",
    "1. Calculate prior probabilities\n",
    "2. Calculate conditional probabilities\n",
    "3. Multiply to find the more likely outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "data = {\n",
    "    'Example': [1, 2, 3, 4, 5, 6, 7],\n",
    "    'Positive Mammogram': ['Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No'],\n",
    "    'Family History': ['Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No'],\n",
    "    'Alcohol': ['Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No'],\n",
    "    'Cancer': ['Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No']\n",
    "}\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Displaying DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "Note in the preceding example, the probability of not having cancer was zero because there were no instances of individuals with cancer that also had a family history in our data - but it's quite likely we just didn't have enough data.  In order to address this, we'd like to replace zeros with a very small value - this would mean that all other probabilities would need to shrink a little bit.  This process is called _smoothing_.\n",
    "\n",
    "#### Laplacian Smoothing:\n",
    "\n",
    "Laplacian Smoothing (also called add-one smoothing) is a technique used to address the problem of zero probabilities in categorical data modeling, including Naive Bayes classification. Laplacian Smoothing works by adjusting the probability calculation to assume that each possible combination of feature and class has been observed at least once.\n",
    "\n",
    "The adjusted probability $P'$ with Laplacian Smoothing is calculated as follows:\n",
    "\n",
    "$$P'(x|C) = \\frac{f + 1}{N + k}$$\n",
    "\n",
    "Where:\n",
    "- $ P'(x|C) $ is the adjusted probability of feature $ x $ given class $ C $.\n",
    "- $ f $ is the observed frequency of $ x $ with class $ C $ in the training data.\n",
    "- $ N $ is the total number of observations of class $ C $ in the training data.\n",
    "- $ k $ is the number of possible values (categories) of the feature.\n",
    "\n",
    "### Exercise with Family History Feature:\n",
    "\n",
    "Use LaPlacian smoothing to calculate the probabilities for the `family history` feature in the preceding data.\n",
    "\n",
    "### Continuous variables\n",
    "\n",
    "\n",
    "When working with continuous variables in a Naive Bayes classifier, there are mainly two methods:\n",
    "1. **Discretization**: Transforming continuous variables into discrete ones, usually by creating bins or intervals.\n",
    "   - **Equal Width Binning**: The range of the variable is divided into equally sized bins.\n",
    "   - **Equal Frequency Binning**: Bins are created such that each bin has approximately the same number of observations.\n",
    "   \n",
    "2. **Probability Density Estimation**: Assuming a certain distribution for the continuous variable (e.g., Normal distribution) and using the probability density function (PDF) to estimate probabilities.\n",
    "\n",
    "In the following, I'll show you how to use probability density to calculate probabilities.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Given the following dataset (introduced previously), we have \"Taxable Income\" as a continuous variable. \n",
    "\n",
    "| Tid | Refund | Marital Status | Taxable Income (K) | Cheat |\n",
    "|-----|--------|----------------|--------------------|-------|\n",
    "| 1   | Yes    | Single         | 125                | No    |\n",
    "| 2   | No     | Married        | 100                | No    |\n",
    "| 3   | No     | Single         | 70                 | No    |\n",
    "| 4   | Yes    | Married        | 120                | No    |\n",
    "| 5   | No     | Divorced       | 95                 | Yes   |\n",
    "| 6   | No     | Married        | 60                 | No    |\n",
    "| 7   | Yes    | Divorced       | 220                | No    |\n",
    "| 8   | No     | Single         | 85                 | Yes   |\n",
    "| 9   | No     | Married        | 75                 | No    |\n",
    "| 10  | No     | Single         | 90                 | Yes   |\n",
    "\n",
    "\n",
    "First, we will assume that \"Taxable Income\" follows a Normal distribution and use the Probability Density Function (PDF) to estimate probabilities (note that we can check this assumption!).\n",
    "\n",
    "The Normal distribution is defined by two parameters: the mean (μ) and the standard deviation (σ). For each class (Cheat/No Cheat), we need to estimate these parameters for \"Taxable Income\".\n",
    "\n",
    "Let's find the parameters for the \"No Cheat\" class:\n",
    "\n",
    "- We will filter out the instances where Cheat = 'No'.\n",
    "- Calculate the mean and standard deviation of \"Taxable Income\" for these instances.\n",
    "\n",
    "##### Calculating Parameters for \"No Cheat\" class\n",
    "For instance, let’s say we have filtered out the following Taxable Incomes for \"No Cheat\": [125, 100, 70, 120, 60, 220, 75]\n",
    "- Mean (μ): $\\frac{125 + 100 + 70 + 120 + 60 + 220 + 75}{7}$\n",
    "- Standard Deviation (σ): It’s the square root of the variance, where variance is the average of the squared differences from the Mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'Tid': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Refund': ['Yes', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No'],\n",
    "    'Marital Status': ['Single', 'Married', 'Single', 'Married', 'Divorced', 'Married', 'Divorced', 'Single', 'Married', 'Single'],\n",
    "    'Taxable Income (K)': [125, 100, 70, 120, 95, 60, 220, 85, 75, 90],\n",
    "    'Cheat': ['No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "result = df.groupby(\"Cheat\")[\"Taxable Income (K)\"].agg([np.mean,np.std])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Once we have the parameters, we can calculate the probability density of any new incoming instance of taxable income using the Probability Density Function for a Normal Distribution:\n",
    "\n",
    "$$ f(x | μ, σ) = \\frac{1}{σ \\sqrt{2π}} e^{-\\frac{(x-μ)^2}{2σ^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we can visualize the normal curves corresponding to this definition like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Separate the data into two DataFrames: one for 'Cheat' and one for 'No Cheat'\n",
    "df_cheat = df[df['Cheat'] == 'Yes']\n",
    "df_no_cheat = df[df['Cheat'] == 'No']\n",
    "\n",
    "# Calculate the mean and standard deviation for each class\n",
    "mean_cheat, std_cheat = df_cheat['Taxable Income (K)'].mean(), df_cheat['Taxable Income (K)'].std()\n",
    "mean_no_cheat, std_no_cheat = df_no_cheat['Taxable Income (K)'].mean(), df_no_cheat['Taxable Income (K)'].std()\n",
    "\n",
    "# Define the range of taxable income\n",
    "x = np.linspace(min(df['Taxable Income (K)']), max(df['Taxable Income (K)']), 1000)\n",
    "\n",
    "# Plot the PDFs\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=x, y=norm.pdf(x, mean_cheat, std_cheat), label='Cheat')\n",
    "sns.lineplot(x=x, y=norm.pdf(x, mean_no_cheat, std_no_cheat), label='No Cheat')\n",
    "plt.title('Probability Density Functions for Cheat and No Cheat Classes')\n",
    "plt.xlabel('Taxable Income (K)')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose we get a new instance with a Taxable Income of 120K, and we want to know the probability of this instance being in the \"No Cheat\" class, we would substitute $x = 110$, $μ$, and $σ \\approx 54.6$ of \"No Cheat\" class in the above equation to get the probability density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assume mean and std have been calculated as mean_no_cheat and std_no_cheat\n",
    "# For a new instance with taxable income = 120\n",
    "stats.norm.pdf(120, loc=mean_no_cheat, scale=std_no_cheat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing normality\n",
    "\n",
    "There are a number of ways to check if data is normally distributed.  \n",
    "\n",
    "Sure, assessing the normality of a data distribution is often a crucial step, especially when employing methods that make assumptions about the data distribution, like the Gaussian Naïve Bayes. Here are a few methods to check the normality of your data.\n",
    "\n",
    "#### 1. **Visual Inspection:**\n",
    "   - **Histogram**: Plotting a histogram of your dataset can provide a good approximation of the underlying distribution. A bell-shaped curve usually signifies a normal distribution.\n",
    "   - **Q-Q Plot (Quantile-Quantile plot)**: A Q-Q plot helps you to visually check if the data follows a normal distribution. If the data is normally distributed, the points in the Q-Q plot will fall in a straight line.\n",
    "\n",
    "#### 2. **Statistical Tests:**\n",
    "   - **Shapiro-Wilk Test**: This test is widely used for testing the normality of a data set and is effective for small sample sizes. A p-value less than the chosen alpha level (e.g., 0.05) indicates that the data does not follow a normal distribution.\n",
    "   - **Anderson-Darling Test**: This is another statistical test used to test if a data sample belongs to a population with a specific distribution (normal, in this case).\n",
    "   - **Kolmogorov-Smirnov Test**: This non-parametric test compares a sample with a reference probability distribution. It’s more suitable for large samples.\n",
    "\n",
    "#### 3. **Descriptive Statistics:**\n",
    "   - **Skewness and Kurtosis**: These are numerical measures that can suggest the symmetry and tailedness of a distribution, respectively. For a normal distribution, both skewness and kurtosis are 0. However, they are more useful for detecting non-normality than confirming normality.\n",
    "\n",
    "Here's an example of how you might implement some of these in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = df['Taxable Income (K)']\n",
    "\n",
    "# Histogram\n",
    "plt.hist(data, bins='auto', color='blue', alpha=0.7, rwidth=0.85)\n",
    "plt.title('Histogram to check for Normality')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Q-Q Plot\n",
    "stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot to check for Normality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test_stat, shapiro_p_value = stats.shapiro(data)\n",
    "if shapiro_p_value < 0.05:\n",
    "    print(\"The data is not normally distributed.\")\n",
    "else:\n",
    "    print(\"The data is normally distributed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-normal distributions\n",
    "\n",
    "If your data is not normal, and you are uncertain what kind of distribution it falls into, you can adopt a non-parametric method to estimate your probabilities.  One approach is to use a kernel density estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n_samples = 1000\n",
    "x = np.linspace(-5, 5, n_samples)\n",
    "\n",
    "# Generate a standard normal distribution\n",
    "normal_pdf = norm.pdf(x, loc=0, scale=1)\n",
    "\n",
    "# Generate a sawtooth wave\n",
    "sawtooth_wave = 0.5 * (1 + np.mod(x, 2))\n",
    "\n",
    "# Combine the normal distribution with the sawtooth wave\n",
    "combined_data = normal_pdf * sawtooth_wave\n",
    "\n",
    "# Sample from the custom distribution\n",
    "data_samples = np.random.choice(x, size=n_samples, p=combined_data/combined_data.sum())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histogram of the sampled data\n",
    "sns.histplot(data_samples, kde=False, stat='density', bins=30, label='Histogram', color='skyblue', alpha=0.6)\n",
    "\n",
    "# Plot KDE\n",
    "sns.kdeplot(data_samples, bw_adjust=0.3, label='KDE', color='orange', linestyle='dashed')\n",
    "\n",
    "# Fit and plot normal PDF to the sampled data\n",
    "mean, std = norm.fit(data_samples)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mean, std)\n",
    "plt.plot(x, p, 'k', linewidth=2, label='Normal PDF')\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Comparison of KDE and Normal PDF against Histogram')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from a KDE of a distribution requires a few steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Example Data\n",
    "data_samples = np.random.normal(0, 1, 1000)\n",
    "\n",
    "# Create KDE\n",
    "# Create KDE\n",
    "kde = gaussian_kde(data_samples)\n",
    "\n",
    "# Generate a range of points\n",
    "x = np.linspace(min(data_samples), max(data_samples), 1000)\n",
    "\n",
    "# Compute the PDF at each point in the range and then compute the CDF\n",
    "pdf_vals = kde.evaluate(x)\n",
    "cdf_vals = np.cumsum(pdf_vals) * (x[1] - x[0])  # Assuming x is uniformly spaced\n",
    "\n",
    "# Perform inverse transform sampling\n",
    "new_samples = np.interp(np.random.uniform(0, 1, 1000), cdf_vals, x)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data_samples, kde=True, stat='density', label='Original Data', alpha=0.5, color='blue')\n",
    "sns.histplot(new_samples, kde=True, stat='density', label='Sampled from KDE', alpha=0.5, color='red')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that all the methods mentioned above have their own limitations, and in practice, it’s often recommended to use them in combination to thoroughly assess the normality of your data. Additionally, it’s crucial to remember that real-world data often deviate from idealized normal distributions, so the goal is usually to ascertain whether the data is close enough to normal for the purposes of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Naive Bayes in Scikit Learn\n",
    "\n",
    "While the underlying theory of Naive Bayes may be a bit daunting, running Naive Bayes in sklearn is quite straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize our classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train our classifier\n",
    "model = gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "preds = model.predict(X_test)\n",
    "print(\"Model Accuracy:\", (preds == y_test).sum() / y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several different versions of Naive Bayes implemented in SKLearn:\n",
    "\n",
    "1. **MultinomialNB**: This is suitable for discrete data and is used for features that represent counts or frequency of occurrence of events. Often used in text classification.\n",
    "   \n",
    "2. **BernoulliNB**: Designed for binary/boolean features, which can be especially useful for binary representation of feature presence or absence (like a one-hot encoded feature).\n",
    "   \n",
    "3. **ComplementNB**: This is suitable for imbalanced datasets and is a modification of MultinomialNB.\n",
    "\n",
    "4. **CategoricalNB**: It is designed for categorical features, and it assumes that each feature has its own categorical distribution.\n",
    "\n",
    "**Differences among them:**\n",
    "\n",
    "- **GaussianNB** assumes that continuous features follow a normal distribution.\n",
    "- **MultinomialNB**, **ComplementNB**, and **BernoulliNB** are suitable for discrete count data, with each having different suitability based on the nature of the data (binary, discrete counts, etc.).\n",
    "- **CategoricalNB** is specialized for handling categorical features.\n",
    "\n",
    "\n",
    "Note that all the Naive Bayes algorithms in scikit-learn are parametric. They assume a specific form for the likelihood function P(X|Y) (normal distribution for GaussianNB, multinomial distribution for MultinomialNB, etc.). Scikit-learn does not currently offer a non-parametric Naive Bayes algorithm out of the box that would allow for more flexible estimation of P(X|Y) without assuming a specific distribution, such as by using kernel density estimation.\n",
    "\n",
    "However, you can potentially extend or implement your own version of a non-parametric Naive Bayes classifier using Kernel Density Estimation (KDE) or other non-parametric density estimation techniques for handling continuous data. Keep in mind that this would involve more complexity and would require a good understanding of both the Naive Bayes algorithm and non-parametric density estimation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The [Federalist Papers](https://www.gutenberg.org/ebooks/18_) are a set of documents written by Alexander Hamilton, John Jay, and James Madison.  One commonly use data analytics example involves trying to infer the authorship for a set of papers with 'disputed' provenance based on the frequency of terms used in the documents.\n",
    "\n",
    "Use Naive Bayes and a Decision Tree to solve this problem.  Be sure to split the test data (marked 'dispt') out of the data before training your model. Which appears to work better.  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/federalistpapers.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
