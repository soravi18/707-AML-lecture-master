{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical method used for binary classification tasks, which predicts the probability that a given input belongs to a certain class (labelled as 1) versus another class (labelled as 0). It's one of the simplest and most widely used classification algorithms, especially suited for problems where you have a linear decision boundary between the two classes.\n",
    "\n",
    "### The Equation\n",
    "\n",
    "The core of logistic regression is the logistic function, which is used to model the probability that a given input belongs to the class labelled as 1. The logistic function, often called the sigmoid function, outputs a value between 0 and 1, which can be interpreted as a probability. The logistic regression model equation can be expressed as:\n",
    "\n",
    "$$ p(YU=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n)}} $$\n",
    "\n",
    "- $ p(Y=1) $ is the predicted probability that the target variable $Y$ belongs to class 1.\n",
    "- $ e $ is the base of the natural logarithm.\n",
    "- $ \\beta_0, \\beta_1, ..., \\beta_n $ are the coefficients of the model, where $ \\beta_0 $ is the intercept term, and $ \\beta_1, ..., \\beta_n $ are the coefficients for the respective features $ X_1, ..., X_n $.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border: solid #805AD5 4px; border-radius: 4px; padding:0.7em; width:90%'>\n",
    "\n",
    "\n",
    "**The Logistic Function**\n",
    "\n",
    "The general form of the logistic function is:\n",
    "\n",
    "$$ p(X) = \\frac{1}{1 + e^{-(\\alpha + \\beta X)}} $$\n",
    "\n",
    "The following example illustrates the logistic function graphically:\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logistic_function(x, alpha, beta):\n",
    "    \"\"\"\n",
    "    Logistic function to calculate probabilities.\n",
    "    x: input feature\n",
    "    alpha: intercept term\n",
    "    beta: coefficient term\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-(alpha + beta*x)))\n",
    "\n",
    "# Graph 1: Modifying alpha (intercept term)\n",
    "x = np.linspace(-10, 10, 200)  # Generate 200 points between -10 and 10\n",
    "alphas = [-5, 0, 5]  # Different values of alpha to shift the curve\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for alpha in alphas:\n",
    "    plt.plot(x, logistic_function(x, alpha, 1), label=f'alpha = {alpha}')\n",
    "\n",
    "plt.title('Effect of Modifying Alpha on the Logistic Function')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Graph 2: Modifying beta (coefficient term)\n",
    "betas = [0.5, 1, 2]  # Different values of beta to modify the slope\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for beta in betas:\n",
    "    plt.plot(x, logistic_function(x, 0, beta), label=f'beta = {beta}')\n",
    "\n",
    "plt.title('Effect of Modifying Beta on the Logistic Function')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border: solid #805AD5 4px; border-radius: 4px; padding:0.7em; width:90%'>\n",
    "\n",
    "**The Logistic Function (cont'd)**\n",
    "\n",
    "1. **Effect of Modifying Alpha ($\\alpha$)**:\n",
    "   - The first graph illustrates how changing the intercept term $\\alpha$ shifts the logistic curve along the x-axis.\n",
    "   - Increasing $\\alpha$ shifts the curve to the right, indicating that higher x values are needed to achieve the same probability.\n",
    "   - Decreasing $\\alpha$ shifts the curve to the left, lowering the x value threshold for achieving higher probabilities.\n",
    "\n",
    "2. **Effect of Modifying Beta ($\\beta$)**:\n",
    "   - The second graph shows the effect of changing the slope coefficient $\\beta$ on the steepness of the logistic curve.\n",
    "   - A higher $\\beta$ value makes the curve steeper, indicating a more sensitive response to changes in x.\n",
    "   - A lower $\\beta$ value results in a flatter curve, indicating a less sensitive response to x changes.\n",
    "\n",
    "These graphs demonstrate how the logistic function \"squashes\" linear combinations of inputs into the [0, 1] range, enabling the calculation of probabilities. Adjusting $\\alpha$ and $\\beta$ can fine-tune the model's response to different feature values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function: Binary Cross-Entropy\n",
    "\n",
    "The loss function used in logistic regression is the binary cross-entropy, also known as log loss. It measures the performance of a classification model whose output is a probability value between 0 and 1. The loss increases as the predicted probability diverges from the actual label. For a dataset with $ N $ observations, the loss function can be written as:\n",
    "\n",
    "$$ L(\\beta) = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p(x_i)) + (1 - y_i) \\log(1 - p(x_i))] $$\n",
    "\n",
    "- $ y_i $ is the actual label of the $i^{th}$ observation, which can be 0 or 1.\n",
    "- $ p(x_i) $ is the predicted probability that the $i^{th}$ observation belongs to class 1.\n",
    "- The sum runs over all $N$ observations in the dataset.\n",
    "\n",
    "The goal of logistic regression is to find the set of coefficients $ \\beta $ that minimize this loss function.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border: solid #805AD5 4px; border-radius: 4px; padding:0.7em; width:90%'>\n",
    "\n",
    "### **Understanding Loss Functions**\n",
    "\n",
    "In machine learning, a **loss function**, also known as a cost function, is a critical concept that quantifies the difference between the predicted values by a model and the actual values in the training data. It essentially measures how well the model is performing, with a lower value indicating a better fit to the data. The goal of training a machine learning model is to find the model parameters (like weights in linear regression or a neural network) that minimize this loss function.\n",
    "\n",
    "### Why It's Important\n",
    "- **Guides Model Training**: The loss function guides the optimization process (like gradient descent) by providing a metric to minimize.\n",
    "- **Model Evaluation**: It helps in evaluating the performance of the model, with different models and hyperparameters being compared based on their loss.\n",
    "\n",
    "### Examples of Loss Functions\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: Commonly used in regression tasks. It calculates the average of the squares of the differences between the predicted and actual values.\n",
    "   - **Equation**: $ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "   - **Context**: Here, $y_i$ is the actual value, $\\hat{y}_i$ is the predicted value, and $n$ is the number of observations.\n",
    "   - **Use Case**: Ideal for regression problems where you want to penalize larger errors more than smaller ones.\n",
    "\n",
    "2. **Binary Cross-Entropy**: Often used in binary classification tasks. It measures the distance between two probability distributions - the actual label distribution and the predicted probabilities.\n",
    "   - **Equation**: $ \\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] $\n",
    "   - **Context**: $y_i$ is the actual label (0 or 1), and $\\hat{y}_i$ is the predicted probability of the instance being in class 1.\n",
    "   - **Use Case**: Useful when modeling problems where each observation is independent and belongs to one of two classes.\n",
    "\n",
    "\n",
    "The choice of loss function depends on the specific machine learning task (e.g., regression, classification) and the distribution of the target variable. Selecting the appropriate loss function is fundamental to effectively training and evaluating machine learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Logistic Regression Algorithm\n",
    "\n",
    "1. **Initialization**: The model coefficients ($ \\beta $) are usually initialized to 0 or a small random number.\n",
    "2. **Model Training**: During training, logistic regression uses an optimization algorithm (like gradient descent) to minimize the loss function. This involves making predictions using the current coefficients, calculating the error by comparing the predicted probabilities with the actual class labels, and then adjusting the coefficients based on this error.\n",
    "3. **Updating Coefficients**: The coefficients are updated in a direction that reduces the loss. The learning rate determines the size of the step taken in this direction at each iteration.\n",
    "4. **Convergence**: The process continues iteratively until the model converges, meaning the reduction in the loss function becomes negligible with further iterations, or until a specified number of iterations is reached.\n",
    "\n",
    "Certainly! Below is a conceptual explanation of learning algorithms, focusing on gradient descent, accompanied by Python code to generate graphs that illustrate key concepts such as the impact of the gradient and learning rate on the learning process.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border: solid #805AD5 4px; border-radius: 4px; padding:0.7em; width:90%'>\n",
    "\n",
    "### **Learning Algorithms and Gradient Descent**\n",
    "\n",
    "Learning algorithms are procedures that a machine learning model uses to learn from data. One of the most fundamental learning algorithms is **Gradient Descent**, which is used to minimize the loss function of a model by iteratively moving towards the minimum of the loss function.\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Gradient**: Represents the slope of the loss function at any point and indicates the direction of steepest ascent. In gradient descent, we're interested in the negative gradient, which points towards the direction of steepest descent.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gradient descent update function\n",
    "def gradient_descent_update(start, learning_rate, gradient_func, steps=10):\n",
    "    x = start\n",
    "    path = [start]\n",
    "    for _ in range(steps):\n",
    "        grad = gradient_func(x)\n",
    "        x -= learning_rate * grad\n",
    "        path.append(x)\n",
    "    return np.array(path)\n",
    "\n",
    "# Gradient functions\n",
    "def shallow_gradient(x): return 0.1 * x  # Shallow gradient\n",
    "def steep_gradient(x): return 2 * x  # Steeper gradient\n",
    "\n",
    "# Learning rate and starting point\n",
    "learning_rate = 0.1\n",
    "start_point = 8\n",
    "\n",
    "# Generate paths\n",
    "path_shallow = gradient_descent_update(start_point, learning_rate, shallow_gradient, steps=10)\n",
    "path_steep = gradient_descent_update(start_point, learning_rate, steep_gradient, steps=10)\n",
    "\n",
    "# Setup subplots with shared y-axis\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6), sharey=True)  # sharey=True for shared y-axis\n",
    "\n",
    "# Plot for shallow gradient\n",
    "axs[0].plot(np.linspace(-10, 10, 400), 0.05 * np.linspace(-10, 10, 400)**2, label='Loss Function with Shallow Gradient')\n",
    "axs[0].plot(path_shallow, 0.05 * path_shallow**2, marker='o', label='Path of Gradient Descent')\n",
    "axs[0].set_title('Convergence with Shallow Gradient')\n",
    "axs[0].set_xlabel('Parameter Value')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot for steeper gradient\n",
    "axs[1].plot(np.linspace(-10, 10, 400), np.linspace(-10, 10, 400)**2, label='Loss Function with Steeper Gradient')\n",
    "axs[1].plot(path_steep, path_steep**2, marker='o', label='Path of Gradient Descent')\n",
    "axs[1].set_title('Convergence with Steeper Gradient')\n",
    "axs[1].set_xlabel('Parameter Value')\n",
    "# axs[1].set_ylabel('Loss')  # Not needed due to shared y-axis\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"warning\" style='background-color:#E9D8FD; color: #69337A; border: solid #805AD5 4px; border-radius: 4px; padding:0.7em; width:90%'>\n",
    "\n",
    "- **Learning Rate**: Determines the size of the steps taken towards the minimum. Affects the convergence of the algorithm.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume a simple gradient descent update\n",
    "def gradient_descent_update(start, learning_rate, steps=10):\n",
    "    x = start\n",
    "    path = [start]\n",
    "    for _ in range(steps):\n",
    "        grad = 2 * x  # Derivative of x^2\n",
    "        x -= learning_rate * grad\n",
    "        path.append(x)\n",
    "    return np.array(path)\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.95]  # Example learning rates\n",
    "start_point = -8\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, label='Loss Function')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    path = gradient_descent_update(start_point, lr)\n",
    "    plt.plot(path, path**2, marker='o', label=f'LR: {lr}')\n",
    "\n",
    "plt.title('Impact of Learning Rate on Convergence')\n",
    "plt.xlabel('Parameter Value')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Logistic Regression is a powerful yet simple classification algorithm that hinges on the concept of probability and odds. It's widely used for binary classification tasks and serves as a foundation for understanding more complex algorithms. Its interpretability, ease of implementation, and efficacy in linearly separable datasets make it a staple algorithm for many machine learning practitioners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on the Iris Data\n",
    "\n",
    "The following example illustrates how logistic regression works with two classes from the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "list(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)  # extra code – it's a bit too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target.head(3)  # note that the instances are not shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.data[[\"petal width (cm)\"]].values\n",
    "y = iris.target_names[iris.target] == 'virginica'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]\n",
    "\n",
    "plt.figure(figsize=(8, 3))  # extra code – not needed, just formatting\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n",
    "         label=\"Not Iris virginica proba\")\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\n",
    "plt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n",
    "         label=\"Decision boundary\")\n",
    "\n",
    "# extra code – this section beautifies and saves Figure 4–23\n",
    "plt.arrow(x=decision_boundary, y=0.08, dx=-0.3, dy=0,\n",
    "          head_width=0.05, head_length=0.1, fc=\"b\", ec=\"b\")\n",
    "plt.arrow(x=decision_boundary, y=0.92, dx=0.3, dy=0,\n",
    "          head_width=0.05, head_length=0.1, fc=\"g\", ec=\"g\")\n",
    "plt.plot(X_train[y_train == 0], y_train[y_train == 0], \"bs\")\n",
    "plt.plot(X_train[y_train == 1], y_train[y_train == 1], \"g^\")\n",
    "plt.xlabel(\"Petal width (cm)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrates the impact of moving the decision boundary\n",
    "\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris.target_names[iris.target] == 'virginica'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(C=2, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# for the contour plot\n",
    "x0, x1 = np.meshgrid(np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "                     np.linspace(0.8, 2.7, 200).reshape(-1, 1))\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]  # one instance per point on the figure\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "\n",
    "# for the decision boundary\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -((log_reg.coef_[0, 0] * left_right + log_reg.intercept_[0])\n",
    "             / log_reg.coef_[0, 1])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \"bs\")\n",
    "plt.plot(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \"g^\")\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.text(3.5, 1.27, \"Not Iris virginica\", color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris virginica\", color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling multiple classes\n",
    "\n",
    "Logistic Regression is inherently a binary classifier, designed to predict the probability of an instance belonging to one of two classes. However, it can be extended to handle multiclass classification problems through techniques like **One-vs-Rest (OvR)** and **Multinomial Logistic Regression** (also calls **softmax regression**). By default, LogisticRegression in sklearn uses Softmax regression.\n",
    "\n",
    "### One-vs-Rest (OvR)\n",
    "One-vs-Rest, also known as One-vs-All (OvA), is a strategy for using binary classification algorithms, like Logistic Regression, for multiclass classification tasks. The basic idea is to train a separate classifier for each class, where each classifier predicts whether an instance belongs to its 'own' class versus 'all other' classes.\n",
    "\n",
    "- **How It Works**:\n",
    "  - If there are $N$ classes, $N$ separate Logistic Regression models are trained.\n",
    "  - Each model is trained to distinguish instances of its own class from instances of all other classes.\n",
    "  - For a given instance, all $N$ models make a prediction, and the model with the highest predicted probability assigns its class to the instance.\n",
    "\n",
    "### Multinomial Logistic Regression\n",
    "Multinomial Logistic Regression, also known as Softmax Regression, extends Logistic Regression to handle multiple classes directly, without having to train multiple binary classifiers.\n",
    "\n",
    "- **How It Works**:\n",
    "  - Instead of using the sigmoid function (which outputs probabilities for two classes), the softmax function is used to compute the probabilities of the multiple classes.\n",
    "  - The softmax function converts the scores (logits) for each class into probabilities by taking the exponential of each score and then normalizing these exponentials by dividing by their sum. This ensures that the probabilities of all classes sum up to 1.\n",
    "  - The model is trained to maximize the likelihood of the true class by adjusting the weights using a generalization of the logistic loss function, suitable for multiple classes.\n",
    "\n",
    "### The Softmax Function\n",
    "For a given instance $x$, the probability that it belongs to class $k$ out of $K$ classes is given by:\n",
    "\n",
    "$$ P(y=k | x) = \\frac{e^{(x^T \\beta_k)}}{\\sum_{j=1}^{K} e^{(x^T \\beta_j)}} $$\n",
    "\n",
    "- $x^T \\beta_k$ is the score (logit) of class $k$ for instance $x$.\n",
    "- $e^{(x^T \\beta_k)}$ is the exponential of the score, ensuring non-negative values.\n",
    "- The denominator is the sum of exponentials of all class scores, normalizing the probabilities.\n",
    "\n",
    "### Loss Function for Multinomial Logistic Regression\n",
    "The loss function used is the cross-entropy loss, generalized for multiple classes. It measures the difference between the predicted probabilities and the actual class labels across all classes and instances.\n",
    "\n",
    "### Conclusion\n",
    "Logistic Regression can be effectively used for multiclass classification by employing strategies like One-vs-Rest or by using the Multinomial Logistic Regression approach. The choice between these methods depends on the specific problem, the dataset, and the performance metrics of interest. Multinomial Logistic Regression is particularly neat as it integrates the entire multiclass prediction into a single model, providing a probabilistic interpretation for multiclass predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Softmax Regression in sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "\n",
    "# Note that 'C' is a 'regularization' parameter which helps to prevent overfitting.\n",
    "softmax_reg = LogisticRegression(C=30, random_state=42)\n",
    "softmax_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg.predict_proba([[5, 2]]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This illustrates the behavior of the softmax function.  Note  that the decision boundary is constructed from a set of line segments.\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "custom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n",
    "\n",
    "x0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "                     np.linspace(0, 3.5, 200).reshape(-1, 1))\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=\"hot\")\n",
    "plt.clabel(contour, inline=1)\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.axis([0.5, 7, 0, 3.5])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
